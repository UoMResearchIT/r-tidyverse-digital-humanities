---
title: Manipulating tibbles with dplyr
teaching: 40
exercises: 15
questions:
- "How can I manipulate tibbles without repeating myself?"
objectives:
- " To be able to use the six main `dplyr` data manipulation 'verbs' with pipes."
keypoints:
- "Use the `dplyr` package to manipulate tibbles."
- "Use `select()` to choose variables from a tibbles."
- "Use `filter()` to choose data based on values."
- "Use `group_by()` and `summarize()` to work with subsets of data."
- "Use `mutate()` to create new variables."
source: Rmd
---

```{r, echo=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("05-")
```

In the previous episode we used the `readr` package to load tabular data into a tibble within R.  The `readr` package is part of a family of packages known as the   [tidyverse](http://tidyverse.org/).  The tidyverse packages are designed to work well together; they provide a modern and streamlined approach to data-analysis, and deal with some of the idiosyncrasies of base R.


This loads the most commonly used packages in the tidyverse; we used `readr` in the previous episode.  We will cover all of the other main packages, with the exception of `purrr` in this course. There are other [libraries included](https://github.com/tidyverse/tidyverse) but these are less widely used, and must be loaded manually if they are required; these aren't covered in this course. 

The data we'll be using for the rest of the course is taken from Twitter. The data contains information on how often various new "slang" words were used in each state of the USA, over a period of time.  The data are taken from [GRIEVE, J., NINI, A., & GUO, D. (2017). Analyzing lexical emergence in Modern American English online. English Language and Linguistics, 21(1), 99-127. doi:10.1017/S1360674316000113
](https://www.cambridge.org/core/journals/english-language-and-linguistics/article/analyzing-lexical-emergence-in-modern-american-english-online-1/73E2D917856BE39ACD9EE3789E2BE597).

Let's dive in and look at how we can use the tidyverse to analyse and, in a couple of episodes' time,  plot data from twitter.    At [the start of the course]({{ page.root}}/02-project-intro), you should have copied the file `twitterData.csv` to your `data` directory.     Take a look at it using a text editor such as notepad.   The first line contains variable names, and values are separated by commas.  Each record starts on a new line. 

The data are in what's often referred to as "tidy" format; each observation is on a new line, and each column represents a variable.  This is what's sometimes known as "long" format.   The tidyverse is geared up towards working with data in tidy format.  Often (almost always in fact), your data won't be in the "shape" you need to analyse it. Transforming and cleaning your data is often one of the most time consuming (and frustrating) parts of the analysis process.   For these reasons, and because it's very problem specific we won't spend much time on it today.
 
FIXME - more here. Mention tidyr?  Or include section on data cleaning at the end? But it introduces so many new concepts, it might be counter productive.

As we did with the [previous episode]({{ page.root }}/03-loading-data-into-R) we use the `read_csv()` function to load the comma separated file. Let's make a new script (using the file menu), and load the tidyverse: (in the previous episode we only loaded `readr`; since we'll be using several packages in the tidyverse, we load them all)

```{r}
library("tidyverse")
twitterData <- read_csv("./data/twitterData.csv")
```

As we discussed in the [previous episode]({{ page.root }}/03-loading-data-into-R), variables in R can be character, integer, double, etc.   A tibble (and R's built in equivalent; the data-frame) require that all the values in a particular column have the same data type.  The `read_csv()` function will attempt to infer the data type of each column, and prints the column types it has guessed to the screen.  If the wrong column types have been generated, you can pass the `col_types=` option to `read_csv()`.  

For example, if we wanted to load the `date` column as a character string, we would use:

```{r, eval=FALSE}
twitterData <- read_csv("data/twitterData.csv", 
                             col_types = cols(
                               date = col_character(),
                               stateCode = col_character(),
                               word = col_character(),
                               cases = col_double(),
                               dataDay = col_double(),
                               Region = col_character(),
                               Division = col_character()
                             ) )
```

> ## Setting column types
> 
> Try reading a file using the `read_csv()` defaults (i.e. guessing column types).
> If this fails you can cut and paste the guessed column specification, and modify
> this with the correct column types.  It is good practice to do this anyway; it makes
> the data types of your columns explicit, and will help protect you if the format 
> of your data changes.
{: .callout}


You may notice from the column specification that the date column of the data has bene read in as a `col_date()`.  R has special data types for handling dates, and "date times" (e.g. `r lubridate::ymd_hms("2018-01-01 00:00:01")`).  Dates and date times are awkward to handle in any programming language (including R); things like different ways of writing dates (day/month/year or month/day/year?) time zones, leap years (and leap seconds) complicate things.  The `lubridate` package in the tidyverse makes dates a bit easier to handle.  This isn't loaded by default.  We will be using one of the functions from it later in this episode, so we'll load it now. Add the following to the top of your script:

```{r}
library(lubridate)
```


## Manipulating tibbles 

Manipulation of tibbles means many things to many researchers. We often
select only certain observations (rows) or variables (columns). We often group the
data by a certain variable(s), or calculate summary statistics.

## The `dplyr` package

The  [`dplyr`](https://cran.r-project.org/web/packages/dplyr/dplyr.pdf)
package is part of the tidyverse.  It provides a number of very useful functions for manipulating tibbles (and their base-R cousin, the `data.frame`) 
in a way that will reduce repetition, reduce the probability of making
errors, and probably even save you some typing. 

We will cover:

1. selecting variables with `select()`
2. subsetting observations with `filter()`
3. grouping observations with `group_by()`
4. generating summary statistics using `summarize()`
5. generating new variables using `mutate()`
6. Sorting tibbles using `arrange()`
7. chaining operations together using pipes `%>%` 

## Using `select()`

If, for example, we wanted to move forward with only a few of the variables in
our tibble we use the `select()` function. This will keep only the
variables you select.

```{r}
dateWordState <- select(twitterData, date, word, stateCode)
print(dateWordState)
```

Select will select _columns_ of data.  What if we want to select rows that meet certain criteria?  

## Using `filter()`

The `filter()` function is used to select rows of data.  For example, to select only data relating to the north east states:

```{r}
twitterDataNE <- filter(twitterData, Region == "Northeast")
print(twitterDataNE)
```

Only rows of the data where the condition (i.e. `Region == "Northeast"`) is `TRUE` are kept.  Note that we use `==` to test for equality.

We can use numeric tests in the `filter()` function too.  For example, to only keep rows where at a word was tweeted at least once:

```{r}
twitterData %>% 
  filter(cases >= 1)
```

We can use the same idea to select tweets that occured on, before or after a certain date.  We need a way of providing the date to R in a way it can understand.  If we want all the tweets that occured on or after 1st Jan 2014, we might try something like:

```{r}
twitterData %>% 
  filter(date >= 01012014)
```

But this doesn't work.  The `dmy()` function in the lubridate function lets us specify a date to R. It is very liberal in how we format the date for example:

```{r}
dmy(01012014)
dmy("1 Jan 2014")
dmy("1/1/2014")
```

will all work.  There are also `mdy()` and `ymd()` functions if you prefer to specify the parts of the date in a different order.
(If these commands don't work, check you have loaded the `lubridate` package with `library(lubridate)`)

So, to only keep tweets on or after 1 Jan 2014 we can use:
```{r}
twitterData %>% 
  filter(date >= dmy("1 Jan 2014"))
```



## Using pipes and dplyr

We've now seen how to choose certain columns of data (using `select()`) and certain rows of data (using `filter()`).  In an analysis we often want to do both of these things (and many other things, like calculating summary statistics, which we'll come to shortly).    How do we combine these?

There are several ways of doing this; the method we will learn about today is using _pipes_.  

The pipe operator `%>%` lets us pipe the output of one command into the next.   This allows us to build up a data-processing pipeline.  This approach has several advantages:

* We can build the pipeline piecemeal - building the pipeline step-by-step is easier than trying to 
perform a complex series of operations in one go
* It is easy to modify and reuse the pipeline
* We don't have to make temporary tibbles as the analysis progresses

> ## Pipelines and the shell
>
> If you're familiar with the Unix shell, you may already have used pipes to
> pass the output from one command to the next.  The concept is the same, except
> the shell uses the `|` character rather than R's pipe operator `%>%`
{: .callout}


> ## Keyboard shortcuts and getting help
> 
> The pipe operator can be tedious to type.  In Rstudio pressing <kbd>Ctrl</kbd> + <kbd>Shift</kbd>+<kbd>M</kbd> under
> Windows / Linux will insert the pipe operator.  On the mac, use <kbd>&#8984;</kbd> + <kbd>Shift</kbd>+<kbd>M</kbd>.
>
> We can use tab completion to complete variable names when entering commands.
> This saves typing and reduces the risk of error.
> 
> RStudio includes a helpful "cheat sheet", which summarises the main functionality
> and syntax of `dplyr`.  This can be accessed via the
> help menu --> cheatsheets --> data transformation with dplyr. 
>
{: .callout}

Let's rewrite the select command example using the pipe operator:

```{r}
dateWordStateCode<- twitterData %>% 
  select(date, word, stateCode)
print(dateWordStateCode)
```

To help you understand why we wrote that in that way, let's walk through it step
by step. First we summon the twitterData tibble and pass it on, using the pipe
symbol `%>%`, to the next step, which is the `select()` function. In this case
we don't specify which data object we use in the `select()` function since in
gets that from the previous pipe. 

What if we wanted to combine this with the filter example? I.e. we want to select the date, word and state code, but only for countries in the north east?  We can join these two operations using a pipe; feeding the output of one command directly into the next:


```{r}
NorthEastData <- twitterData %>% 
  filter(Region == "Northeast") %>% 
  select(date, word, stateCode)
print(NorthEastData)
```

Note that the order of these operations matters; if we reversed the order of the `select()` and `filter()` functions, the `stateCode` variable wouldn't exist in the data-set when we came to apply the filter.  

What about if we wanted to match more than one item?  To do this we use the `%in%` operator:

```{r}
twitterData %>% 
  filter(stateCode %in% c("WY", "UT", "CO", "AZ", "NM"))

```


> ## Another way of thinking about pipes
>
> It might be useful to think of the statement
> ```{r, eval=FALSE}
> NorthEastData <- twitterData %>% 
>  filter(Region == "Northeast") %>% 
>  select(date, word, stateCode)
> ```
>  as a sentence, which we can read as
> "take the twitter data *and then* `filter` records where `Region == "Northeast"`
> *and then* `select` the date, word and stateCode
> 
> We can think of the `filter()` and `select()` functions as verbs in the sentence; 
> they do things to the data flowing through the pipeline.  
>
{: .callout}

> ## Splitting your commands over multiple lines
> 
> It's generally a good idea to put one command per line when
> writing your analyses.  This makes them easier to read.   When
> doing this, it's important that the `%>%` goes at the _end_ of the
> line, as in the example above.  If we put it at the beginning of a line, e.g.:
> 
> ```{r}
> twitterData 
>   %>% filter(stateCode %in% c("WY", "UT", "CO", "AZ", "NM"))
>
> ```
> 
> the first line makes a valid R command.  R will then treat the next line 
> as a new command, which won't work.
{: .callout}


> ## Challenge 1
>
> Write a single command (which can span multiple lines and includes pipes) that
> will produce a tibble that has the values of  `cases`, `stateCode`
> and `dataDay`, for the countries in the south, but not for other regions.  How many rows does your tibble  
> have? (You can use the `nrow()` function to find out how many rows are in a tibble.)
>
> > ## Solution to Challenge 1
> >```{r}
> > SouthBae <- twitterData %>% 
> >   filter(Region == "South") %>% 
> >   filter(word == "bae") %>% 
> >   select(cases, stateCode, dataDay) 
> > nrow(SouthBae)
> > ```
> > As with last time, first we pass the twitterData tibble to the `filter()`
> > function, then we pass the filtered version of the twitterData tibble  to the
> > `select()` function. **Note:** The order of operations is very important in this
> > case. If we used 'select' first, filter would not be able to find the variable
> > Region since we would have removed it in the previous step.
> {: .solution}
{: .challenge}


## Sorting tibbles

The `arrange()` function will sort a tibble by one or more of the variables in it:

```{r}
twitterData %>% 
  filter(word == "anime") %>%
  arrange(cases)
```
We can use the `desc()` function to sort a variable in reverse order:

```{r}
twitterData %>% 
  filter(word == "anime") %>%
  arrange(desc(cases))
```

## Generating new variables

The `mutate()` function lets us add new variables to our tibble.  It will often be the case that these are variables we _derive_ from existing variables in the data-frame. 

As an example, we can calculate the proportion of all tokens that were each of the words we are studying:

```{r}
twitterData %>% 
  mutate(wordProp = cases / totalTokens)
```

The dplyr cheat sheet contains many useful functions which can be used with dplyr.  This can be found in the help menu of RStudio. You will use one of these functions in the next challenge.

## Challenge 2

In this challenge we'll calculate a cumulative sum of how often the word "anime" occurs over time in the state of New York.   We haven't talked about how to make a cumulative sum yet; take a look at the dplyr cheat sheet for this part of the question.

There are a few steps you'll need to go through to do this.  When you're doing this, it's a really good idea to build you analysis pipeline command by command and check it's doing what you think it is after each step.  

The first thing we'll need to do is filter the rows of data we want. It's also a good idea to just select the columns of data we need for the rest of the exercise at this point. It looks like the data are sorted by date, but we don't _know_ this for sure, so it'll be a good idea to make sure.  Then we'll need to make a new column containing the cumulative sum.  

As this is quite a long challenge, the solution is split into several parts:

## Solution - the rows we want

The first thing to do is to get the rows of data we want.  We use the `filter()` function for this:

```{r}
twitterData %>% 
  filter(stateCode == "NY") %>% 
  filter(word == "anime")
```

{: .solution}

## Solution - the columns we want

We can extend the pipeline with a `select()` to get the columns we need for the rest of the challenge:

```{r}
twitterData %>% 
  filter(stateCode == "NY") %>% 
  filter(word == "anime") %>% 
  select(date, cases)
```

{: .solution}

## Solution - getting things in order

It's a good idea to check the data are in date order:

```{r}
twitterData %>% 
  filter(stateCode == "NY") %>% 
  filter(word == "anime") %>% 
  select(date, cases) %>% 
  arrange(date)
```
{: .solution}


## Solution - calculating the cumulative sum

The last thing to do is to calculate the cumulative sum.  We need to make a new column, so we use `mutate()`; the dplyr cheat sheet lists (some of) the functions we can use with mutate, including `cumsum()`. 


```{r}
twitterData %>% 
  filter(stateCode == "NY") %>% 
  filter(word == "anime") %>% 
  select(date, cases) %>% 
  arrange(date) %>% 
  mutate(cumulativeUse = cumsum(cases))
```

{: .solution}
{: .challenge}

## Calculating summary statistics

We often wish to calculate a summary statistic (the mean, standard deviation, etc.)
for a variable.  We frequently want to calculate a separate summary statistic for several
groups of data (e.g. for each state or region).    We can calculate a summary statistic
for the whole data-set using the dplyr's `summarise()` function:

```{r}
twitterData %>% 
  filter(word == "anime") %>% 
  summarise(totalAnime = sum(cases))
```

Tells us how often "anime" occured in the whole data set

To generate summary statistics for each value of another variable we use the 
`group_by()` function:

```{r}
twitterData %>% 
  group_by(word) %>% 
  summarise(total = sum(cases))
```


## Statistics revision

If you need to revise or learn about statistical concepts, the University Library's "My Learning Essentials" team have produced a site [Start to Finish:Statistics](https://www.escholar.manchester.ac.uk/learning-objects/mle/packages/statistics/) which covers important statistical concepts.

{: .callout}

## Challenge 3

For each day in January 2014 calculate the number of times each word occured

Hint - first filter the data so that only observations in January 2014 are included.  Then group by date and word

## Solution
```{r}
twitterData %>% 
  filter(date >= dmy("1 Jan 2014")) %>% 
  filter(date < dmy("1 Feb 2014")) %>% 
  group_by(date, word) %>% 
  summarise(totalUse = sum(cases))
```

{: .solution}
{: .challenge}

## `count()` and `n()`
A very common operation is to count the number of observations for each
group. The `dplyr` package comes with two related functions that help with this.


If we need to use the number of observations in calculations, the `n()` function
is useful. For instance, if we wanted to see how many observations there were for each word we could use:

```{r}
twitterData %>% 
  group_by(word) %>% 
  summarise(numObs = n())
```

There's a shorthand for this; the count function:

```{r}
twitterData %>% 
  count(word) 
```
We can optionally sort the results in descending order by adding `sort=TRUE`:



## Connect mutate with logical filtering: `ifelse()`

When creating new variables, we can hook this with a logical condition. A simple combination of 
`mutate()` and `ifelse()` facilitates filtering right where it is needed: in the moment of creating something new.
This easy-to-read statement is a fast and powerful way of discarding certain data (even though the overall dimension
of the tibble will not change) or for updating values depending on this given condition.

The `ifelse()` function takes three parameters.  The first it the logical test.  The second is the value to use if the test is TRUE for that observation, and the third is the value to use if the test is FALSE.

For example, if we wanted to take the log of all observations where the word was mentioned at least once, and use `NA` otherwise, we could use:

```{r}
twitterData %>% 
  select(date, stateCode, word, cases) %>% 
  mutate(logcases = ifelse(cases > 0, log(cases), NA))
```


> ## Equivalent functions in base R
>
> In this course we've taught the tidyverse.  You are likely come across
> code written others in base R.  You can find a guide to some base R functions
> and their tidyverse equivalents [here](http://www.significantdigits.org/2017/10/switching-from-base-r-to-tidyverse/),
> which may be useful when reading their code.
>
{: .callout}
## Other great resources

* [Data Wrangling tutorial](https://suzan.rbind.io/categories/tutorial/) - an excellent four part tutorial covering selecting data, filtering data, summarising and transforming your data.
* [R for Data Science](http://r4ds.had.co.nz/)
* [Data Wrangling Cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)
* [Introduction to dplyr](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) - this is the package vignette.  It can be viewed within R using `vignette(package="dplyr", "dplyr")`
* [Data wrangling with R and RStudio](https://www.rstudio.com/resources/webinars/data-wrangling-with-r-and-rstudio/)
